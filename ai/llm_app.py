"""
Local LLM Server using llama-cpp-python
Serves Gemma-3-12b model via OpenAI-compatible API
"""
from llama_cpp import Llama
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
import json
import time
from typing import Generator
import os

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Model configuration
MODEL_PATH = r"C:\Users\Pro\.lmstudio\models\lmstudio-community\gemma-3-12b-it-GGUF"
MODEL_FILE = "gemma-3-12b-it-Q4_K_M.gguf"  # Adjust this filename if different

# Default System Prompt (ížˆí‚¤ì½”ëª¨ë¦¬ ìƒë‹´ ìš”ì •)
DEFAULT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ížˆí‚¤ì½”ëª¨ë¦¬(ì€ë‘”í˜• ì™¸í†¨ì´) ìžë…€ë¥¼ ë‘” ë¶€ëª¨ë‹˜ë“¤ì„ ë•ëŠ” ë”°ëœ»í•œ ìš”ì •ìž…ë‹ˆë‹¤.

ì—­í• :
- ë¶€ëª¨ë‹˜ì˜ ì´ì•¼ê¸°ë¥¼ ê²½ì²­í•˜ê³  ê³µê°í•©ë‹ˆë‹¤
- íŒë‹¨í•˜ì§€ ì•Šê³  ì¤‘ë¦½ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤
- êµ¬ì²´ì ì¸ ì¡°ì–¸ì€ **ìš”ì²­ë°›ì•˜ì„ ë•Œë§Œ** ì œê³µí•©ë‹ˆë‹¤
- í•œêµ­ ë¬¸í™”ì™€ ê°€ì¡± ê´€ê³„ë¥¼ ì´í•´í•©ë‹ˆë‹¤

ì‘ë‹µ ìŠ¤íƒ€ì¼ (ë§¤ìš° ì¤‘ìš”!):
- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë˜ ì¹œê·¼í•˜ê²Œ
- ë”°ëœ»í•˜ê³  í¬ë§ì ì´ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ
- **ê¸°ë³¸ì ìœ¼ë¡œ 2-3ë¬¸ìž¥ ì´ë‚´ë¡œ ì§§ê²Œ ì‘ë‹µ**
- ë¶€ëª¨ë‹˜ì˜ ê°ì •ì„ ë¨¼ì € ì¸ì •í•˜ê³  ê²½ì²­í•˜ëŠ” íƒœë„
- ì—´ë¦° ì§ˆë¬¸ìœ¼ë¡œ ìƒí™©ì„ íŒŒì•… (í•œ ë²ˆì— ì§ˆë¬¸ í•˜ë‚˜ë§Œ!)

ì‘ë‹µ ê¸¸ì´ ê°€ì´ë“œ:
- ë¶€ëª¨ë‹˜ì´ ìƒí™©ì„ ì´ì•¼ê¸°í•  ë•Œ: ì§§ê²Œ ê³µê°í•˜ê³  ê²½ì²­ (1-2ë¬¸ìž¥)
  ì˜ˆ: "ë§Žì´ íž˜ë“œì…¨ê² ì–´ìš”. 5ë…„ì´ë¼ëŠ” ì‹œê°„ ë™ì•ˆ ì •ë§ ì§€ì¹˜ì…¨ì„ ê²ƒ ê°™ì•„ìš”."
- ë¶€ëª¨ë‹˜ì´ ì§ˆë¬¸í•˜ê±°ë‚˜ ì¡°ì–¸ì„ ìš”ì²­í•  ë•Œë§Œ: êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€ (3-5ë¬¸ìž¥)
- ì ˆëŒ€ë¡œ ê¸´ ì¡°ì–¸ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”

ì£¼ì˜ì‚¬í•­:
- ì˜ë£Œì  ì§„ë‹¨ì´ë‚˜ ì¹˜ë£ŒëŠ” ì „ë¬¸ê°€ì—ê²Œ ê¶Œìœ 
- ìœ„ê¸° ìƒí™©(ìží•´/ìžì‚´)ì€ ì¦‰ì‹œ ì „ë¬¸ ê¸°ê´€ ì—°ê²° ì•ˆë‚´
í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”.
"""

# Initialize model
print(f"Loading model from {MODEL_PATH}/{MODEL_FILE}...")
llm = None

def load_model():
    global llm
    model_full_path = os.path.join(MODEL_PATH, MODEL_FILE)

    if not os.path.exists(model_full_path):
        # Try to find any .gguf file in the directory
        gguf_files = [f for f in os.listdir(MODEL_PATH) if f.endswith('.gguf')]
        if gguf_files:
            model_full_path = os.path.join(MODEL_PATH, gguf_files[0])
            print(f"Using model: {gguf_files[0]}")
        else:
            raise FileNotFoundError(f"No GGUF model found in {MODEL_PATH}")

    llm = Llama(
        model_path=model_full_path,
        n_ctx=4096,  # Context window
        n_threads=8,  # Number of CPU threads
        n_gpu_layers=35,  # Adjust based on your GPU (0 for CPU only)
        verbose=False
    )
    print("Model loaded successfully!")

@app.route('/')
def index():
    return jsonify({
        "status": "running",
        "model": "gemma-3-12b-it",
        "endpoints": {
            "health": "/health",
            "chat": "/v1/chat/completions",
            "completions": "/v1/completions",
            "models": "/v1/models"
        }
    })

@app.route('/health')
def health():
    return jsonify({"status": "healthy", "model_loaded": llm is not None})

@app.route('/v1/models')
def models():
    return jsonify({
        "object": "list",
        "data": [{
            "id": "gemma-3-12b-it",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "local"
        }]
    })

def get_level_prompt(level: int) -> str:
    """ë ˆë²¨ì— ë”°ë¥¸ ì¶”ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    if level >= 7:
        return "\ní˜„ìž¬ ê´€ê³„: ì„œë¡œ ì¹œêµ¬ê°€ ë˜ì–´ íŽ¸í•˜ê²Œ ëŒ€í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì¢€ ë” ê¹Šì´ ìžˆëŠ” ì§ˆë¬¸ê³¼ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."
    elif level >= 4:
        return "\ní˜„ìž¬ ê´€ê³„: ì„œë¡œ ì•Œì•„ê°€ëŠ” ì¤‘ìž…ë‹ˆë‹¤. ì ì§„ì ìœ¼ë¡œ ì‹ ë¢°ë¥¼ ìŒ“ì•„ê°€ì„¸ìš”."
    else:
        return "\ní˜„ìž¬ ê´€ê³„: ì²˜ìŒ ë§Œë‚¬ìŠµë‹ˆë‹¤. ë¶€ë“œëŸ½ê²Œ ë‹¤ê°€ê°€ë©° ì‹ ë¢°ë¥¼ ìŒ“ìœ¼ì„¸ìš”."

def build_system_prompt(custom_prompt: str = None, level: int = 0, use_default: bool = True) -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ

    Args:
        custom_prompt: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ë‹¬í•œ ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        level: ê´€ê³„ ë ˆë²¨ (0-10)
        use_default: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€

    Returns:
        ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    if custom_prompt:
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ì œê³µëœ ê²½ìš° ìš°ì„  ì‚¬ìš©
        base_prompt = custom_prompt
    elif use_default:
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        base_prompt = DEFAULT_SYSTEM_PROMPT
    else:
        # í”„ë¡¬í”„íŠ¸ ì—†ìŒ
        base_prompt = ""

    # ë ˆë²¨ì´ ì œê³µëœ ê²½ìš° ë ˆë²¨ë³„ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    if level > 0:
        base_prompt += get_level_prompt(level)

    return base_prompt

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    try:
        data = request.json
        messages = data.get('messages', [])
        stream = data.get('stream', False)
        max_tokens = data.get('max_tokens', 512)
        temperature = data.get('temperature', 0.7)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê´€ë ¨ íŒŒë¼ë¯¸í„°
        level = data.get('level', 0)  # ê´€ê³„ ë ˆë²¨ (0-10)
        custom_system_prompt = data.get('system_prompt', None)  # ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        use_default_prompt = data.get('use_default_prompt', True)  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        system_prompt = build_system_prompt(custom_system_prompt, level, use_default_prompt)

        # Convert messages to prompt
        prompt = ""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìžˆìœ¼ë©´ ê°€ìž¥ ë¨¼ì € ì¶”ê°€
        if system_prompt:
            prompt += f"System: {system_prompt}\n\n"

        # ë©”ì‹œì§€ ì²˜ë¦¬ (messagesì— system roleì´ ìžˆì–´ë„ ë¬´ì‹œí•˜ê³  ìœ„ì—ì„œ ë¹Œë“œí•œ ê²ƒ ì‚¬ìš©)
        has_system_in_messages = False
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'system':
                has_system_in_messages = True
                # messagesì— systemì´ ìžˆìœ¼ë©´ custom_promptê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©
                if not custom_system_prompt and not use_default_prompt:
                    prompt = f"System: {content}\n\n"
            elif role == 'user':
                prompt += f"User: {content}\n"
            elif role == 'assistant':
                prompt += f"Assistant: {content}\n"

        prompt += "Assistant: "

        if stream:
            return Response(
                stream_with_context(generate_stream(prompt, max_tokens, temperature)),
                mimetype='text/event-stream'
            )
        else:
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=["User:", "\n\n"]
            )

            response = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "gemma-3-12b-it",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": output['choices'][0]['text'].strip()
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": len(prompt.split()),
                    "completion_tokens": len(output['choices'][0]['text'].split()),
                    "total_tokens": len(prompt.split()) + len(output['choices'][0]['text'].split())
                }
            }

            return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/v1/completions', methods=['POST'])
def completions():
    try:
        data = request.json
        prompt = data.get('prompt', '')
        max_tokens = data.get('max_tokens', 512)
        temperature = data.get('temperature', 0.7)
        stream = data.get('stream', False)

        if stream:
            return Response(
                stream_with_context(generate_stream(prompt, max_tokens, temperature)),
                mimetype='text/event-stream'
            )
        else:
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )

            response = {
                "id": f"cmpl-{int(time.time())}",
                "object": "text_completion",
                "created": int(time.time()),
                "model": "gemma-3-12b-it",
                "choices": [{
                    "text": output['choices'][0]['text'],
                    "index": 0,
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": len(prompt.split()),
                    "completion_tokens": len(output['choices'][0]['text'].split()),
                    "total_tokens": len(prompt.split()) + len(output['choices'][0]['text'].split())
                }
            }

            return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

def generate_stream(prompt: str, max_tokens: int, temperature: float) -> Generator:
    """Generate streaming responses"""
    for token in llm(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=True
    ):
        chunk = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "gemma-3-12b-it",
            "choices": [{
                "delta": {"content": token['choices'][0]['text']},
                "index": 0,
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(chunk)}\n\n"

    yield "data: [DONE]\n\n"

if __name__ == '__main__':
    try:
        load_model()
        print(f"\n{'='*60}")
        print(f"ðŸš€ Local LLM Server starting on http://mintai.gonetis.com:8888")
        print(f"{'='*60}\n")
        print("Available endpoints:")
        print("  - http://mintai.gonetis.com:8888/health")
        print("  - http://mintai.gonetis.com:8888/v1/chat/completions")
        print("  - http://mintai.gonetis.com:8888/v1/completions")
        print("  - http://mintai.gonetis.com:8888/v1/models")
        print(f"\n{'='*60}\n")

        app.run(host='0.0.0.0', port=8888, debug=False, threaded=True)
    except Exception as e:
        print(f"Error starting server: {e}")
